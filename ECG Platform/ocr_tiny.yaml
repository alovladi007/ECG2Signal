
model:
  name: transformer_ocr
  d_model: 256
  nhead: 8
  num_layers: 4

training:
  epochs: 200
  batch_size: 64
  learning_rate: 0.0005
