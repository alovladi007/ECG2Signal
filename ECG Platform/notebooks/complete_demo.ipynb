{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG2Signal Complete Demo\n",
    "\n",
    "This notebook demonstrates the complete ECG image-to-signal conversion pipeline.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Loading and preprocessing ECG images\n",
    "2. Grid detection and calibration\n",
    "3. Layout detection and OCR\n",
    "4. Signal segmentation and extraction\n",
    "5. Signal reconstruction and post-processing\n",
    "6. Clinical feature extraction\n",
    "7. Quality assessment\n",
    "8. Exporting to multiple formats\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies (if needed)\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample ECG Image\n",
    "\n",
    "First, let's create a realistic synthetic ECG image with grid and waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ecg2signal.training.data_synth import synth_ecg, render\n",
    "\n",
    "# Generate synthetic 12-lead ECG signals\n",
    "print(\"Generating synthetic ECG signals...\")\n",
    "signals = synth_ecg.generate_12lead_ecg(\n",
    "    duration=10.0,  # 10 seconds\n",
    "    heart_rate=75,  # BPM\n",
    "    sampling_rate=500  # Hz\n",
    ")\n",
    "\n",
    "print(f\"Generated signals for {len(signals)} leads\")\n",
    "print(f\"Signal duration: {len(signals['I'])/500:.1f} seconds\")\n",
    "print(f\"Sampling rate: 500 Hz\")\n",
    "\n",
    "# Render to image with grid\n",
    "print(\"\\nRendering ECG to image with grid...\")\n",
    "ecg_image = render.render_ecg_to_image(\n",
    "    signals=signals,\n",
    "    paper_speed=25.0,  # mm/s\n",
    "    gain=10.0,  # mm/mV\n",
    "    add_grid=True,\n",
    "    add_labels=True,\n",
    "    add_noise=True,\n",
    "    image_size=(3000, 2400)\n",
    ")\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.imshow(ecg_image, cmap='gray')\n",
    "plt.title('Synthetic ECG Image with Grid', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Image generated: {ecg_image.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Preprocess\n",
    "\n",
    "Load the ECG image and apply preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ecg2signal.io import image_io\n",
    "from ecg2signal.preprocess import detect_page, dewarp, denoise\n",
    "\n",
    "# Step 2.1: Page detection\n",
    "print(\"Step 2.1: Detecting page boundaries...\")\n",
    "page_detected = detect_page.detect_and_crop_page(ecg_image)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "axes[0].imshow(ecg_image, cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(page_detected, cmap='gray')\n",
    "axes[1].set_title('Page Detected & Cropped')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 2.2: Perspective correction\n",
    "print(\"\\nStep 2.2: Correcting perspective distortion...\")\n",
    "dewarped = dewarp.correct_perspective(page_detected)\n",
    "\n",
    "# Step 2.3: Denoising\n",
    "print(\"Step 2.3: Denoising image...\")\n",
    "denoised = denoise.denoise_image(dewarped)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "axes[0].imshow(dewarped, cmap='gray')\n",
    "axes[0].set_title('After Dewarp')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(denoised, cmap='gray')\n",
    "axes[1].set_title('After Denoising')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Preprocessing complete\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Grid Detection and Calibration\n",
    "\n",
    "Detect the ECG grid and calculate pixel-to-physical unit calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ecg2signal.preprocess import grid_detect, scale_calibrate\n",
    "\n",
    "# Detect grid\n",
    "print(\"Detecting grid lines...\")\n",
    "grid_info = grid_detect.detect_grid(denoised)\n",
    "\n",
    "if grid_info:\n",
    "    print(f\"âœ… Grid detected!\")\n",
    "    print(f\"   Horizontal spacing: {grid_info['horizontal_spacing']:.2f} pixels\")\n",
    "    print(f\"   Vertical spacing: {grid_info['vertical_spacing']:.2f} pixels\")\n",
    "    print(f\"   Detected {len(grid_info.get('horizontal_lines', []))} horizontal lines\")\n",
    "    print(f\"   Detected {len(grid_info.get('vertical_lines', []))} vertical lines\")\n",
    "    \n",
    "    # Visualize grid detection\n",
    "    grid_vis = denoised.copy()\n",
    "    if len(grid_vis.shape) == 2:\n",
    "        grid_vis = cv2.cvtColor(grid_vis, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Draw detected lines\n",
    "    for line in grid_info.get('horizontal_lines', [])[:20]:  # Show first 20\n",
    "        cv2.line(grid_vis, (0, line), (grid_vis.shape[1], line), (0, 255, 0), 2)\n",
    "    for line in grid_info.get('vertical_lines', [])[:20]:  # Show first 20\n",
    "        cv2.line(grid_vis, (line, 0), (line, grid_vis.shape[0]), (255, 0, 0), 2)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(cv2.cvtColor(grid_vis, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Grid Detection (Green=Horizontal, Blue=Vertical)', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ No grid detected, using default calibration\")\n",
    "    grid_info = None\n",
    "\n",
    "# Calibrate\n",
    "print(\"\\nCalculating calibration...\")\n",
    "calibration = scale_calibrate.calibrate_from_grid(\n",
    "    grid_info,\n",
    "    paper_speed=25.0,  # mm/s\n",
    "    gain=10.0  # mm/mV\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“ Calibration Results:\")\n",
    "print(f\"   Pixels per mm: {calibration.pixels_per_mm:.2f}\")\n",
    "print(f\"   Paper speed: {calibration.paper_speed} mm/s\")\n",
    "print(f\"   Gain: {calibration.gain} mm/mV\")\n",
    "print(f\"   Time resolution: {1000 / (calibration.pixels_per_mm * calibration.paper_speed):.2f} ms/pixel\")\n",
    "print(f\"   Amplitude resolution: {1.0 / (calibration.pixels_per_mm * calibration.gain):.3f} mV/pixel\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Layout Detection\n",
    "\n",
    "Detect the positions of individual lead panels and rhythm strips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ecg2signal.layout import lead_layout, ocr_labels\n",
    "\n",
    "# Detect lead layout\n",
    "print(\"Detecting lead layout...\")\n",
    "layout = lead_layout.detect_lead_layout(denoised)\n",
    "\n",
    "print(f\"âœ… Detected {len(layout)} leads\")\n",
    "print(f\"\\nLead positions:\")\n",
    "for lead_name, bbox in list(layout.items())[:5]:  # Show first 5\n",
    "    print(f\"   {lead_name}: ({bbox['x1']}, {bbox['y1']}) to ({bbox['x2']}, {bbox['y2']})\")\n",
    "\n",
    "# Visualize layout\n",
    "layout_vis = denoised.copy()\n",
    "if len(layout_vis.shape) == 2:\n",
    "    layout_vis = cv2.cvtColor(layout_vis, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "colors = {\n",
    "    'I': (255, 0, 0), 'II': (0, 255, 0), 'III': (0, 0, 255),\n",
    "    'aVR': (255, 255, 0), 'aVL': (255, 0, 255), 'aVF': (0, 255, 255),\n",
    "    'V1': (128, 0, 0), 'V2': (0, 128, 0), 'V3': (0, 0, 128),\n",
    "    'V4': (128, 128, 0), 'V5': (128, 0, 128), 'V6': (0, 128, 128)\n",
    "}\n",
    "\n",
    "for lead_name, bbox in layout.items():\n",
    "    color = colors.get(lead_name, (200, 200, 200))\n",
    "    cv2.rectangle(layout_vis, \n",
    "                  (bbox['x1'], bbox['y1']), \n",
    "                  (bbox['x2'], bbox['y2']), \n",
    "                  color, 3)\n",
    "    cv2.putText(layout_vis, lead_name, \n",
    "                (bbox['x1'] + 5, bbox['y1'] + 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.imshow(cv2.cvtColor(layout_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Lead Layout Detection', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# OCR for metadata\n",
    "print(\"\\nExtracting metadata with OCR...\")\n",
    "metadata = ocr_labels.extract_labels(denoised)\n",
    "print(f\"ðŸ“„ Extracted metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"   {key}: {value}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Segmentation\n",
    "\n",
    "Segment the image into layers: grid, waveforms, and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ecg2signal.segment import separate_layers\n",
    "\n",
    "print(\"Segmenting image layers...\")\n",
    "masks = separate_layers.segment_layers(denoised)\n",
    "\n",
    "print(f\"âœ… Generated masks for: {list(masks.keys())}\")\n",
    "\n",
    "# Visualize masks\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "axes[0, 0].imshow(denoised, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image', fontsize=14)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(masks['grid'], cmap='gray')\n",
    "axes[0, 1].set_title('Grid Mask', fontsize=14)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(masks['waveform'], cmap='gray')\n",
    "axes[1, 0].set_title('Waveform Mask', fontsize=14)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(masks.get('text', np.zeros_like(denoised)), cmap='gray')\n",
    "axes[1, 1].set_title('Text Mask', fontsize=14)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show waveform overlay\n",
    "overlay = denoised.copy()\n",
    "if len(overlay.shape) == 2:\n",
    "    overlay = cv2.cvtColor(overlay, cv2.COLOR_GRAY2BGR)\n",
    "overlay[masks['waveform'] > 128] = [0, 255, 0]  # Highlight waveforms in green\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Detected Waveforms (Green)', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Signal Extraction and Reconstruction\n",
    "\n",
    "Extract waveforms and convert them to calibrated time-series signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ecg2signal.segment import trace_curve\n",
    "from ecg2signal.reconstruct import raster_to_signal, resample, align_leads, postprocess\n",
    "\n",
    "# Extract signals for each lead\n",
    "print(\"Extracting signals from each lead...\")\n",
    "extracted_signals = {}\n",
    "\n",
    "for lead_name, bbox in list(layout.items())[:3]:  # Process first 3 for demo\n",
    "    print(f\"\\nProcessing {lead_name}...\")\n",
    "    \n",
    "    # Extract lead region\n",
    "    lead_mask = masks['waveform'][bbox['y1']:bbox['y2'], bbox['x1']:bbox['x2']]\n",
    "    \n",
    "    # Trace the waveform curve\n",
    "    curve = trace_curve.trace_waveform(lead_mask)\n",
    "    print(f\"  Traced {len(curve)} points\")\n",
    "    \n",
    "    # Convert to signal with calibration\n",
    "    signal = raster_to_signal.pixels_to_signal(\n",
    "        curve,\n",
    "        calibration,\n",
    "        roi_offset=(bbox['x1'], bbox['y1'])\n",
    "    )\n",
    "    print(f\"  Signal length: {len(signal)} samples\")\n",
    "    \n",
    "    # Resample to standard rate (500 Hz)\n",
    "    resampled = resample.resample_signal(signal, target_fs=500.0)\n",
    "    print(f\"  Resampled to: {len(resampled)} samples\")\n",
    "    \n",
    "    # Post-process (filter, baseline removal)\n",
    "    clean_signal = postprocess.postprocess_signal(resampled)\n",
    "    print(f\"  Post-processed: {len(clean_signal)} samples\")\n",
    "    \n",
    "    extracted_signals[lead_name] = clean_signal\n",
    "\n",
    "# Visualize extracted signals\n",
    "fig, axes = plt.subplots(len(extracted_signals), 1, figsize=(15, 4*len(extracted_signals)))\n",
    "if len(extracted_signals) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (lead_name, signal) in enumerate(extracted_signals.items()):\n",
    "    time = np.arange(len(signal)) / 500.0  # Time in seconds\n",
    "    axes[idx].plot(time, signal, 'b-', linewidth=1.5)\n",
    "    axes[idx].set_title(f'Lead {lead_name}', fontsize=14)\n",
    "    axes[idx].set_xlabel('Time (s)', fontsize=12)\n",
    "    axes[idx].set_ylabel('Amplitude (mV)', fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_xlim([0, 2])  # Show first 2 seconds\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Extracted {len(extracted_signals)} signals\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Clinical Feature Extraction\n",
    "\n",
    "Extract clinical intervals and compute heart rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ecg2signal.clinical import intervals\n",
    "\n",
    "# Use first lead for clinical analysis\n",
    "first_lead_name = list(extracted_signals.keys())[0]\n",
    "first_signal = extracted_signals[first_lead_name]\n",
    "\n",
    "print(f\"Extracting clinical intervals from {first_lead_name}...\")\n",
    "clinical_intervals = intervals.extract_intervals(first_signal, fs=500.0)\n",
    "\n",
    "print(f\"\\nâ¤ï¸ Clinical Measurements:\")\n",
    "print(f\"   Heart Rate: {clinical_intervals.get('heart_rate', 'N/A')} BPM\")\n",
    "print(f\"   PR Interval: {clinical_intervals.get('pr_interval', 'N/A')} ms\")\n",
    "print(f\"   QRS Duration: {clinical_intervals.get('qrs_duration', 'N/A')} ms\")\n",
    "print(f\"   QT Interval: {clinical_intervals.get('qt_interval', 'N/A')} ms\")\n",
    "print(f\"   QTc (Corrected): {clinical_intervals.get('qtc_interval', 'N/A')} ms\")\n",
    "\n",
    "# Visualize with annotations\n",
    "if 'r_peaks' in clinical_intervals:\n",
    "    time = np.arange(len(first_signal)) / 500.0\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(time, first_signal, 'b-', linewidth=1.5, label='ECG Signal')\n",
    "    \n",
    "    # Mark R-peaks\n",
    "    r_peaks = clinical_intervals['r_peaks']\n",
    "    plt.plot(time[r_peaks], first_signal[r_peaks], 'ro', \n",
    "             markersize=10, label='R-peaks')\n",
    "    \n",
    "    plt.title(f'Lead {first_lead_name} with R-peak Detection', fontsize=16)\n",
    "    plt.xlabel('Time (s)', fontsize=12)\n",
    "    plt.ylabel('Amplitude (mV)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlim([0, min(5, len(first_signal)/500.0)])  # Show first 5 seconds\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Quality Assessment\n",
    "\n",
    "Assess signal quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ecg2signal.clinical import quality\n",
    "\n",
    "print(\"Assessing signal quality...\")\n",
    "quality_metrics = quality.assess_quality(list(extracted_signals.values()))\n",
    "\n",
    "print(f\"\\nðŸ“Š Quality Metrics:\")\n",
    "print(f\"   Overall Quality: {quality_metrics['overall_quality']:.2%}\")\n",
    "print(f\"   SNR: {quality_metrics.get('snr', 'N/A')} dB\")\n",
    "print(f\"   Baseline Wander: {quality_metrics.get('baseline_wander', 'N/A'):.3f}\")\n",
    "print(f\"   Clipping Detected: {'Yes' if quality_metrics.get('clipping_detected') else 'No'}\")\n",
    "print(f\"   Coverage: {quality_metrics['coverage']:.2%}\")\n",
    "\n",
    "# Visualize quality\n",
    "metrics_names = ['Overall Quality', 'SNR (norm)', 'Coverage']\n",
    "metrics_values = [\n",
    "    quality_metrics['overall_quality'],\n",
    "    min(quality_metrics.get('snr', 20) / 40, 1.0),  # Normalize to 0-1\n",
    "    quality_metrics['coverage']\n",
    "]\n",
    "\n",
    "colors = ['green' if v > 0.8 else 'orange' if v > 0.6 else 'red' for v in metrics_values]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(metrics_names, metrics_values, color=colors, alpha=0.7)\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel('Score', fontsize=12)\n",
    "plt.title('Signal Quality Assessment', fontsize=16)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, metrics_values)):\n",
    "    plt.text(val + 0.02, i, f'{val:.2%}', va='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Export to Multiple Formats\n",
    "\n",
    "Export the extracted signals to various clinical formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ecg2signal.io import wfdb_io, edf_io, fhir, dcm_waveform\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('demo_outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Exporting to multiple formats...\\n\")\n",
    "\n",
    "# 1. WFDB Format\n",
    "print(\"1. Exporting to WFDB (MIT format)...\")\n",
    "wfdb_io.write_wfdb(\n",
    "    extracted_signals,\n",
    "    str(output_dir / 'ecg'),\n",
    "    fs=500.0\n",
    ")\n",
    "print(f\"   âœ… Saved: {output_dir}/ecg.dat, {output_dir}/ecg.hea\")\n",
    "\n",
    "# 2. CSV Format\n",
    "print(\"\\n2. Exporting to CSV...\")\n",
    "df = pd.DataFrame(extracted_signals)\n",
    "df.index.name = 'sample'\n",
    "df.to_csv(output_dir / 'ecg_signals.csv')\n",
    "print(f\"   âœ… Saved: {output_dir}/ecg_signals.csv\")\n",
    "print(f\"   Preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# 3. JSON Format\n",
    "print(\"\\n3. Exporting to JSON...\")\n",
    "json_data = {\n",
    "    'signals': {k: v.tolist() for k, v in extracted_signals.items()},\n",
    "    'sampling_rate': 500.0,\n",
    "    'calibration': {\n",
    "        'paper_speed': calibration.paper_speed,\n",
    "        'gain': calibration.gain,\n",
    "        'pixels_per_mm': calibration.pixels_per_mm\n",
    "    },\n",
    "    'clinical_intervals': clinical_intervals,\n",
    "    'quality_metrics': quality_metrics\n",
    "}\n",
    "\n",
    "with open(output_dir / 'ecg_data.json', 'w') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "print(f\"   âœ… Saved: {output_dir}/ecg_data.json\")\n",
    "\n",
    "# 4. EDF+ Format\n",
    "print(\"\\n4. Exporting to EDF+ format...\")\n",
    "edf_io.write_edf(\n",
    "    extracted_signals,\n",
    "    str(output_dir / 'ecg.edf'),\n",
    "    fs=500.0\n",
    ")\n",
    "print(f\"   âœ… Saved: {output_dir}/ecg.edf\")\n",
    "\n",
    "# 5. FHIR Format\n",
    "print(\"\\n5. Exporting to HL7 FHIR format...\")\n",
    "fhir_obs = fhir.to_fhir_observation(\n",
    "    extracted_signals,\n",
    "    fs=500.0,\n",
    "    patient_id='demo-patient-001'\n",
    ")\n",
    "with open(output_dir / 'ecg_fhir.json', 'w') as f:\n",
    "    json.dump(fhir_obs, f, indent=2)\n",
    "print(f\"   âœ… Saved: {output_dir}/ecg_fhir.json\")\n",
    "\n",
    "# 6. DICOM Waveform\n",
    "print(\"\\n6. Exporting to DICOM Waveform format...\")\n",
    "dcm_waveform.write_dicom_waveform(\n",
    "    extracted_signals,\n",
    "    str(output_dir / 'ecg.dcm'),\n",
    "    fs=500.0,\n",
    "    patient_name='Demo Patient'\n",
    ")\n",
    "print(f\"   âœ… Saved: {output_dir}/ecg.dcm\")\n",
    "\n",
    "print(f\"\\nâœ… All exports complete! Files saved to: {output_dir.absolute()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Complete Pipeline with ECGConverter\n",
    "\n",
    "Now let's use the high-level `ECGConverter` API to process everything in one call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ecg2signal import ECGConverter\n",
    "from ecg2signal.config import Settings\n",
    "\n",
    "# Save our test image\n",
    "test_image_path = output_dir / 'test_ecg.png'\n",
    "cv2.imwrite(str(test_image_path), ecg_image)\n",
    "\n",
    "print(\"Using ECGConverter for complete pipeline...\\n\")\n",
    "\n",
    "# Initialize converter\n",
    "settings = Settings()\n",
    "converter = ECGConverter(settings)\n",
    "\n",
    "# Convert image to signals\n",
    "print(\"Converting ECG image to signals...\")\n",
    "result = converter.convert(\n",
    "    str(test_image_path),\n",
    "    paper_speed=25.0,\n",
    "    gain=10.0,\n",
    "    sample_rate=500\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Conversion complete!\")\n",
    "print(f\"\\nResult summary:\")\n",
    "print(f\"   Number of leads: {len(result.signals)}\")\n",
    "print(f\"   Sampling rate: {result.sample_rate} Hz\")\n",
    "print(f\"   Paper speed: {result.paper_settings.paper_speed} mm/s\")\n",
    "print(f\"   Gain: {result.paper_settings.gain} mm/mV\")\n",
    "\n",
    "if result.intervals:\n",
    "    print(f\"\\n   Heart rate: {result.intervals.get('heart_rate', 'N/A')} BPM\")\n",
    "    print(f\"   QRS duration: {result.intervals.get('qrs_duration', 'N/A')} ms\")\n",
    "\n",
    "if result.quality_metrics:\n",
    "    print(f\"\\n   Quality score: {result.quality_metrics.overall_score:.2%}\")\n",
    "\n",
    "# Export using result object\n",
    "print(\"\\nExporting via result object...\")\n",
    "result.export_wfdb(str(output_dir / 'pipeline_output'))\n",
    "print(f\"âœ… Exported to: {output_dir}/pipeline_output.*\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. âœ… **Generated** synthetic ECG image with realistic grid and waveforms\n",
    "2. âœ… **Preprocessed** image (page detection, dewarping, denoising)\n",
    "3. âœ… **Detected** grid and calibrated pixel-to-physical units\n",
    "4. âœ… **Identified** lead positions and extracted metadata\n",
    "5. âœ… **Segmented** image into grid, waveforms, and text\n",
    "6. âœ… **Reconstructed** digital signals from pixel traces\n",
    "7. âœ… **Extracted** clinical intervals (HR, PR, QRS, QT)\n",
    "8. âœ… **Assessed** signal quality metrics\n",
    "9. âœ… **Exported** to 6 different formats (WFDB, CSV, JSON, EDF, FHIR, DICOM)\n",
    "10. âœ… **Demonstrated** high-level API usage\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Train models** with real ECG data for better accuracy\n",
    "- **Test with real ECG scans** from hospitals or PhysioNet\n",
    "- **Tune parameters** for specific ECG types and qualities\n",
    "- **Deploy** the API for production use\n",
    "- **Integrate** with EHR/EMR systems\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Documentation**: See `docs/` directory\n",
    "- **API Reference**: Run the API and visit `/docs`\n",
    "- **Test Data**: PhysioNet databases (MIT-BIH, PTB-XL)\n",
    "- **Support**: Check GitHub issues or documentation\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Congratulations!** You've completed the full ECG2Signal demo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
